{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access a Term Vector\n",
    "\n",
    "\n",
    "A term vector is information and statistics in the fields of a particular document. Term vectors in Elasticsearch are generated on the fly.\n",
    "\n",
    "\n",
    " \n",
    "## Getting Started\n",
    "\n",
    "In this example, we will use the Elaticsearch Python API. First, we will import and set-up all of the required Python modules and variables we will use later on. Additionally, if you wish to use `curl` instead of the Python API, the corresponding command line function has been commented above each API request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import pandas as pd\n",
    "es = Elasticsearch(urls=['localhost'], port=9200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine what a document in this index looks like. (this operation may take few seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This query will retrieve every document in the index.\n",
    "query = {\n",
    "    'query': {\n",
    "        'match_all': {}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Send a search request to Elasticsearch.\n",
    "# curl -X GET localhost:9200/goma/_search -H 'Content-Type: application/json' -d @query.json\n",
    "res = es.search(index='goma', body=query)\n",
    "\n",
    "# The response is a json object, the listing is nested inside it.\n",
    "# Here we are accessing the first hit in the listing.\n",
    "res['hits']['hits'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the term vector API, let's investigate the term vector for the description field of the document above (id _AV19Sgi4jk6MoKTLfifp_). Note in the call to the `termvectors` method, we explicitly request the term statistics `term_statistics=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# curl -X GET localhost:9200/goma/event/2CmUSmgBOPedV1qM5TF2/_termvectors?term_statistics&fields=description\n",
    "res = es.termvectors(index='goma', doc_type='_doc', id='2CmUSmgBOPedV1qM5TF2', \n",
    "                     fields=['description'], term_statistics=True)\n",
    "\n",
    "# We don't really care that much about the additional info, let's get straight to the point.\n",
    "tv = res['term_vectors']['description']\n",
    "tv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a big json object, so let's break it down into some digestable tables. Firstly, let's take a look at the field statistics.\n",
    "\n",
    " - `doc_count`: document count (how many documents contain this field)\n",
    " - `sum_doc_freq`: sum of document frequencies (the sum of document frequencies for all terms in this field)\n",
    " - `sum_ttf`: sum of total term frequencies (the sum of total term frequencies of each term in this field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tv['field_statistics'], index=['count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More importantly, we can also see the breakdown of the term statistics in the document for each term in the document. These tables omit the `tokens` field, however this is can be used to extract the location of the term in the document.\n",
    "\n",
    " - `term_freq`: term frequency in the field\n",
    " - `doc_freq`: document frequency (the number of documents containing the current term)\n",
    " - `ttf`: sum of total term frequencies (the sum of total term frequencies of each term in this field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = []\n",
    "for term in tv['terms']:\n",
    "    term_info = tv['terms'][term].copy()\n",
    "    del(term_info['tokens'])\n",
    "    term_info.update({'term': term})\n",
    "    terms.append(term_info)\n",
    "df = pd.DataFrame(terms).set_index('term')\n",
    "df[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorted by doc_freq\n",
    "df.sort_values(by='doc_freq', ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorted by term_freq\n",
    "df.sort_values(by='term_freq', ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorted by ttf\n",
    "df.sort_values(by='ttf', ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "\n",
    "Repeat the expoloration of the term vector for a document using the ClueWeb12 sample index you have built in previous activities.\n",
    "\n",
    "#### Exercise 2 -- advanced\n",
    "\n",
    "Using the Clueweb12 sample index, identify two documents that contain a query term of your choice (suggestion: after having chosen a term, query the index to retrieve the top 2 documents that satisfy the query). Then, compute the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) between the two term vectors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
