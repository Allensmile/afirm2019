{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Activity 5: Implementation of a Custom Similarity Model in Elasticsearch\n",
    "\n",
    "In this activity, we are going to implement a custom similarity model to our clueweb12 sample collection. Thus, the pre-requisites and most scripts in this activity are the same as in Activity 1. \n",
    "\n",
    "As noted in: https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-similarity.html\n",
    "Elasticsearch 6.5 allows us to easily define a custom similarity by defining a scripted similarity.\n",
    "\n",
    "In this activity, we are going to follow the given example on how to reimplement TF-IDF similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import builtins\n",
    "#vars(globals()['__builtin__']) is vars(builtins)\n",
    "import gzip\n",
    "import warc\n",
    "import time\n",
    "import glob\n",
    "import lxml.html\n",
    "import re\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "from elasticsearch import Elasticsearch\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to specify the location of the Clueweb12 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid warc path\n"
     ]
    }
   ],
   "source": [
    "warcPath = \"/home/jimmy/data/Clueweb12B_sample/\"\n",
    "\n",
    "# Check the warc Path\n",
    "if not os.path.isdir(warcPath):\n",
    "    print(\"invalid warc path\")\n",
    "else:\n",
    "    print(\"valid warc path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open connection to Elasticsearch (ES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "es0 = Elasticsearch(urls='http://localhost', port=9200, timeout=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify bulk size and max documents in a bulk. For faster indexing, we will index documents in bulks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_size = 4000\n",
    "bulk_count = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define index name and document type for the aquaint index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexName = \"clueweb12_custom\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define our custom similarity named: \"scripted_tfidf\". The two fields in the index (\"title\" and \"body\" fields) will use this custom similarity.\n",
    "\n",
    "Note that the \"weight_script\" will compute  the document-independent part of the score and will be available under the \"weight\" variable. When no \"weight_script\" is provided, weight is equal to 1. The \"weight_script\" has access to the same variables as the \"script\" except doc since it is supposed to compute a document-independent contribution to the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_body = {\n",
    "    \"settings\": {\n",
    "      \"number_of_shards\": 1,\n",
    "      \"number_of_replicas\": 0,\n",
    "      \"analysis\": {\n",
    "        \"analyzer\": {\n",
    "            \"my_english\": {\n",
    "                \"tokenizer\": \"standard\",\n",
    "                \"filter\": [\"lowercase\", \"terrier_stopwords\", \"porter_stem\"]\n",
    "            }\n",
    "        },\n",
    "        \"filter\": {\n",
    "          \"terrier_stopwords\": {\n",
    "              \"type\": \"stop\",\n",
    "              \"stopwords_path\": \"stopwords/terrier-stop.txt\"\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"similarity\": {\n",
    "        \"scripted_tfidf\": {\n",
    "            \"type\": \"scripted\",\n",
    "            \"weight_script\": {\n",
    "              \"source\": \"double idf = Math.log((field.docCount+1.0)/(term.docFreq+1.0)) + 1.0; return query.boost * idf;\"\n",
    "            },\n",
    "            \"script\": {\n",
    "              \"source\": \"double tf = Math.sqrt(doc.freq); double norm = 1/Math.sqrt(doc.length); return weight * tf * norm;\"\n",
    "            }\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "      \"_doc\": {\n",
    "        \"_source\": {\n",
    "            \"enabled\": True\n",
    "        },\n",
    "        \"properties\": {\n",
    "            \"title\": {\n",
    "                 \"type\": \"text\",\n",
    "                 \"similarity\": \"scripted_tfidf\",\n",
    "                 \"analyzer\": \"my_english\"\n",
    "            },\n",
    "            \"body\": {\n",
    "                \"type\": \"text\",\n",
    "                \"similarity\": \"scripted_tfidf\",\n",
    "                \"analyzer\": \"my_english\"\n",
    "            }\n",
    "         }\n",
    "      }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the index based on the specified settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating  clueweb12_custom  index\n",
      " response: '{'index': 'clueweb12_custom', 'acknowledged': True, 'shards_acknowledged': True}'\n"
     ]
    }
   ],
   "source": [
    "if not es0.indices.exists(indexName):\n",
    "    print (\"creating \", indexName, \" index\")\n",
    "    res = es0.indices.create(index=indexName, body=request_body)\n",
    "    print(\" response: '%s'\" % res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the indexing function which will be executed as a parallel process.\n",
    "This function accepts path to a single gziped warc file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_index(fname):\n",
    "    i = 0\n",
    "    totalSize = 0\n",
    "    bulk_data = []\n",
    "    lapTime = time.time()\n",
    "    es = Elasticsearch(urls='http://localhost', port=9200, timeout=600)\n",
    "\n",
    "    print(\"Processing file: {}\".format(fname))\n",
    "    with gzip.open(fname, mode='rb') as gzf:\n",
    "        print(\"Extracted: {}\".format(fname))\n",
    "        WarcTotalDocuments = 0\n",
    "        EmptyDocuments = 0\n",
    "        for record in warc.WARCFile(fileobj=gzf):\n",
    "            if record.header.get('WARC-Type').lower() == 'warcinfo':\n",
    "                WarcTotalDocuments = record.header.get('WARC-Number-Of-Documents')\n",
    "\n",
    "            if record.header.get('WARC-Type').lower() == 'response':\n",
    "                docId = record.header.get('WARC-Trec-ID')\n",
    "                docString = record.payload.read().decode(\"utf-8\", \"ignore\")\n",
    "\n",
    "                htmlStart = docString.find('<html')\n",
    "                if htmlStart < 1:\n",
    "                    htmlStart = docString.find('<HTML')\n",
    "                if htmlStart < 1:\n",
    "                    htmlStart = docString.find('<Html')\n",
    "\n",
    "                if htmlStart < 1:\n",
    "                    EmptyDocuments += 1\n",
    "                else:\n",
    "                    # extract and scrub html string\n",
    "                    htmlString = docString[htmlStart:]\n",
    "                    htmlString = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\xff]', '', htmlString)\n",
    "                    htmlString = re.sub(r'&\\w{4,6};', '', htmlString)\n",
    "                    htmlString = htmlString.replace(\",\", \" \").replace(\"-\", \" \").replace(\".\", \" \")\n",
    "\n",
    "                    #fContent = io.BytesIO(str(htmlString.decode(\"utf-8\", \"ignore\")))\n",
    "                    fContent = io.BytesIO(htmlString.encode(\"utf-8\", \"ignore\"))\n",
    "\n",
    "                    try:\n",
    "                        htmlDoc = lxml.html.parse(fContent)\n",
    "\n",
    "                        # the html.xpath return an array so need to convert it to string with join method\n",
    "                        title = \" \".join(htmlDoc.xpath('//title/text()'))\n",
    "\n",
    "                        rootClean = htmlDoc.getroot()\n",
    "\n",
    "                        body = \" - \"\n",
    "                        try:\n",
    "                            body = rootClean.body.text_content()\n",
    "                            body = ' '.join(word for word in body.split() if word.isalnum())\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        content = title + body\n",
    "                        bulk_meta = {\n",
    "                            \"index\": {\n",
    "                                \"_index\": indexName,\n",
    "                                \"_type\": \"_doc\",\n",
    "                                \"_id\": docId\n",
    "                            }\n",
    "                        }\n",
    "\n",
    "                        bulk_content = {\n",
    "                            'title': title,\n",
    "                            'body': body\n",
    "                        }\n",
    "\n",
    "                        bulk_data.append(bulk_meta)\n",
    "                        bulk_data.append(bulk_content)\n",
    "                        totalSize += (sys.getsizeof(content) / 1024)  # convert from bytes to KiloBytes\n",
    "\n",
    "                        i += 1\n",
    "                        if totalSize > bulk_size or i % bulk_count == 0:\n",
    "                            res = es.bulk(index=indexName, body=bulk_data, refresh=False)\n",
    "                            bulk_data = []\n",
    "                            totalSize = 0\n",
    "                    except:\n",
    "                        print(\"Error processing document: {}\".format(docId))\n",
    "                        raise\n",
    "\n",
    "        if len(bulk_data) > 0:\n",
    "            # index the remainder files\n",
    "            res = es.bulk(index=indexName, body=bulk_data, refresh=False)\n",
    "\n",
    "        print(\"File {0} Completed, Duration: {1} sec, Total: {2}, Processed: {3}, Empty: {4}, Variance: {5}\".\n",
    "               format(fname, time.time() - lapTime, WarcTotalDocuments, str(i), str(EmptyDocuments),\n",
    "                      str(int(WarcTotalDocuments) - i - EmptyDocuments)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traverse all the folders in the original corpus path and parallely process all gzipped warc files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Path: /home/jimmy/data/Clueweb12B_sample/ClueWeb12_00\n",
      "Processing folder: /home/jimmy/data/Clueweb12B_sample/ClueWeb12_00/0001wb\n",
      "Processing file: /home/jimmy/data/Clueweb12B_sample/ClueWeb12_00/0001wb/0001wb-02.warc.gz\n",
      "Processing file: /home/jimmy/data/Clueweb12B_sample/ClueWeb12_00/0001wb/0001wb-04.warc.gz\n",
      "Processing file: /home/jimmy/data/Clueweb12B_sample/ClueWeb12_00/0001wb/0001wb-03.warc.gz\n",
      "Processing file: /home/jimmy/data/Clueweb12B_sample/ClueWeb12_00/0001wb/0001wb-01.warc.gz\n",
      "Processing file: /home/jimmy/data/Clueweb12B_sample/ClueWeb12_00/0001wb/0001wb-00.warc.gz\n",
      "Extracted: /home/jimmy/data/Clueweb12B_sample/ClueWeb12_00/0001wb/0001wb-02.warc.gz\n",
      "Extracted: /home/jimmy/data/Clueweb12B_sample/ClueWeb12_00/0001wb/0001wb-01.warc.gz\n",
      "Extracted: /home/jimmy/data/Clueweb12B_sample/ClueWeb12_00/0001wb/0001wb-03.warc.gz\n",
      "Extracted: /home/jimmy/data/Clueweb12B_sample/ClueWeb12_00/0001wb/0001wb-04.warc.gz\n",
      "Extracted: /home/jimmy/data/Clueweb12B_sample/ClueWeb12_00/0001wb/0001wb-00.warc.gz\n",
      "File /home/jimmy/data/Clueweb12B_sample/ClueWeb12_00/0001wb/0001wb-02.warc.gz Completed, Duration: 9.246980905532837 sec, Total: 2775, Processed: 2704, Empty: 71, Variance: 0\n",
      "File /home/jimmy/data/Clueweb12B_sample/ClueWeb12_00/0001wb/0001wb-03.warc.gz Completed, Duration: 9.248280048370361 sec, Total: 2549, Processed: 2473, Empty: 76, Variance: 0\n",
      "File /home/jimmy/data/Clueweb12B_sample/ClueWeb12_00/0001wb/0001wb-00.warc.gz Completed, Duration: 9.268179416656494 sec, Total: 2788, Processed: 2696, Empty: 92, Variance: 0\n",
      "File /home/jimmy/data/Clueweb12B_sample/ClueWeb12_00/0001wb/0001wb-01.warc.gz Completed, Duration: 9.362834215164185 sec, Total: 2622, Processed: 2561, Empty: 61, Variance: 0\n",
      "File /home/jimmy/data/Clueweb12B_sample/ClueWeb12_00/0001wb/0001wb-04.warc.gz Completed, Duration: 10.78724718093872 sec, Total: 2861, Processed: 2819, Empty: 42, Variance: 0\n"
     ]
    }
   ],
   "source": [
    "warcFolder = glob.glob(warcPath + \"*\")\n",
    "for warcFold in warcFolder:\n",
    "    folders = glob.glob(warcFold + \"/*\")\n",
    "    print(\"Processing Path: {}\".format(warcFold))\n",
    "\n",
    "    for fold in folders:\n",
    "        print(\"Processing folder: {}\".format(fold))\n",
    "        p = multiprocessing.Pool()\n",
    "        resultString = p.map(es_index, glob.glob(fold + \"/*\"))\n",
    "        p.close()\n",
    "        p.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
