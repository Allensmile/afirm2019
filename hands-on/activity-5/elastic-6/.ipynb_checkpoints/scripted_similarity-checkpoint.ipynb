{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Activity 5: Implementation of a Custom Similarity Model in Elasticsearch\n",
    "\n",
    "In this activity, we will implement a custom similarity model for our ClueWeb12 sample collection. \n",
    "Note that in Elasticsearch the similarity function needs to be embedded in the index: thus we will create a new index for the ClueWeb12 sample dataset.\n",
    "The pre-requisites and most scripts in this activity are the same as in Activity 1. \n",
    " \n",
    "Elasticsearch 6.5.x allows us to easily define a custom similarity by defining a scripted similarity, unlike previous versions of Elasticsearch in which a custom similarity plugin needed to be implemented as a Java class.\n",
    "\n",
    "In this activity, we are going to implement a basic TF-IDF similarity function. (note, of course, Elasticsearch already has a TF-IDF similarity function, but TF-IDF provides us a simple use case for practicing implementing a similarity functions through scripts in Elasticsearch).\n",
    "\n",
    "Let's start by importing the libraries required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import builtins\n",
    "#vars(globals()['__builtin__']) is vars(builtins)\n",
    "import gzip\n",
    "import warc\n",
    "import time\n",
    "import glob\n",
    "import lxml.html\n",
    "import re\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "from elasticsearch import Elasticsearch\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to specify the location of the Clueweb12 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put here the path to the Clueweb12B_sample file. \n",
    "#It is best you give the full, absolute path.\n",
    "warcPath = \"~/Clueweb12B_sample/\"\n",
    "\n",
    "# Check the warc Path\n",
    "if not os.path.isdir(warcPath):\n",
    "    print(\"invalid warc path\")\n",
    "else:\n",
    "    print(\"valid warc path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open connection to Elasticsearch (ES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es0 = Elasticsearch(urls='http://localhost', port=9200, timeout=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the bulk size and max documents to process in bulk (For faster indexing, we will index documents in bulks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_size = 4000\n",
    "bulk_count = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a new index name and document type for the clueweb index with the new similarity function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexName = \"clueweb12_custom\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define our custom similarity named `scripted_tfidf`. The two fields in the index (`title` and `body`) will use this custom similarity.\n",
    "\n",
    "Note that the `weight_script` will compute  the document-independent part of the score and will be available under the `weight` variable. When no `weight_script` is provided, the weight is equal to 1 by default. The `weight_script` has access to the same variables as the `script`, except `doc` since it is supposed to compute a document-independent contribution to the score.\n",
    "\n",
    "*This is really the only part of the indexing that differs from what we have already seen in activity 1.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_body = {\n",
    "    \"settings\": {\n",
    "      \"number_of_shards\": 1,\n",
    "      \"number_of_replicas\": 0,\n",
    "      \"analysis\": {\n",
    "        \"analyzer\": {\n",
    "            \"my_english\": {\n",
    "                \"tokenizer\": \"standard\",\n",
    "                \"filter\": [\"lowercase\", \"terrier_stopwords\", \"porter_stem\"]\n",
    "            }\n",
    "        },\n",
    "        \"filter\": {\n",
    "          \"terrier_stopwords\": {\n",
    "              \"type\": \"stop\",\n",
    "              \"stopwords_path\": \"stopwords/terrier-stop.txt\"\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"similarity\": {\n",
    "        \"scripted_tfidf\": {\n",
    "            \"type\": \"scripted\",\n",
    "            \"weight_script\": {\n",
    "              \"source\": \"double idf = Math.log((field.docCount+1.0)/(term.docFreq+1.0)) + 1.0; return query.boost * idf;\"\n",
    "            },\n",
    "            \"script\": {\n",
    "              \"source\": \"double tf = Math.sqrt(doc.freq); double norm = 1/Math.sqrt(doc.length); return weight * tf * norm;\"\n",
    "            }\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "      \"_doc\": {\n",
    "        \"_source\": {\n",
    "            \"enabled\": True\n",
    "        },\n",
    "        \"properties\": {\n",
    "            \"title\": {\n",
    "                 \"type\": \"text\",\n",
    "                 \"similarity\": \"scripted_tfidf\",\n",
    "                 \"analyzer\": \"my_english\"\n",
    "            },\n",
    "            \"body\": {\n",
    "                \"type\": \"text\",\n",
    "                \"similarity\": \"scripted_tfidf\",\n",
    "                \"analyzer\": \"my_english\"\n",
    "            }\n",
    "         }\n",
    "      }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the index based on the specified settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not es0.indices.exists(indexName):\n",
    "    print (\"creating \", indexName, \" index\")\n",
    "    res = es0.indices.create(index=indexName, body=request_body)\n",
    "    print(\" response: '%s'\" % res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the indexing function which will be executed as a parallel process.\n",
    "This function accepts path to a single gziped warc file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_index(fname):\n",
    "    i = 0\n",
    "    totalSize = 0\n",
    "    bulk_data = []\n",
    "    lapTime = time.time()\n",
    "    es = Elasticsearch(urls='http://localhost', port=9200, timeout=600)\n",
    "\n",
    "    print(\"Processing file: {}\".format(fname))\n",
    "    with gzip.open(fname, mode='rb') as gzf:\n",
    "        print(\"Extracted: {}\".format(fname))\n",
    "        WarcTotalDocuments = 0\n",
    "        EmptyDocuments = 0\n",
    "        for record in warc.WARCFile(fileobj=gzf):\n",
    "            if record.header.get('WARC-Type').lower() == 'warcinfo':\n",
    "                WarcTotalDocuments = record.header.get('WARC-Number-Of-Documents')\n",
    "\n",
    "            if record.header.get('WARC-Type').lower() == 'response':\n",
    "                docId = record.header.get('WARC-Trec-ID')\n",
    "                docString = record.payload.read().decode(\"utf-8\", \"ignore\")\n",
    "\n",
    "                htmlStart = docString.find('<html')\n",
    "                if htmlStart < 1:\n",
    "                    htmlStart = docString.find('<HTML')\n",
    "                if htmlStart < 1:\n",
    "                    htmlStart = docString.find('<Html')\n",
    "\n",
    "                if htmlStart < 1:\n",
    "                    EmptyDocuments += 1\n",
    "                else:\n",
    "                    # extract and scrub html string\n",
    "                    htmlString = docString[htmlStart:]\n",
    "                    htmlString = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\xff]', '', htmlString)\n",
    "                    htmlString = re.sub(r'&\\w{4,6};', '', htmlString)\n",
    "                    htmlString = htmlString.replace(\",\", \" \").replace(\"-\", \" \").replace(\".\", \" \")\n",
    "\n",
    "                    #fContent = io.BytesIO(str(htmlString.decode(\"utf-8\", \"ignore\")))\n",
    "                    fContent = io.BytesIO(htmlString.encode(\"utf-8\", \"ignore\"))\n",
    "\n",
    "                    try:\n",
    "                        htmlDoc = lxml.html.parse(fContent)\n",
    "\n",
    "                        # the html.xpath return an array so need to convert it to string with join method\n",
    "                        title = \" \".join(htmlDoc.xpath('//title/text()'))\n",
    "\n",
    "                        rootClean = htmlDoc.getroot()\n",
    "\n",
    "                        body = \" - \"\n",
    "                        try:\n",
    "                            body = rootClean.body.text_content()\n",
    "                            body = ' '.join(word for word in body.split() if word.isalnum())\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        content = title + body\n",
    "                        bulk_meta = {\n",
    "                            \"index\": {\n",
    "                                \"_index\": indexName,\n",
    "                                \"_type\": \"_doc\",\n",
    "                                \"_id\": docId\n",
    "                            }\n",
    "                        }\n",
    "\n",
    "                        bulk_content = {\n",
    "                            'title': title,\n",
    "                            'body': body\n",
    "                        }\n",
    "\n",
    "                        bulk_data.append(bulk_meta)\n",
    "                        bulk_data.append(bulk_content)\n",
    "                        totalSize += (sys.getsizeof(content) / 1024)  # convert from bytes to KiloBytes\n",
    "\n",
    "                        i += 1\n",
    "                        if totalSize > bulk_size or i % bulk_count == 0:\n",
    "                            res = es.bulk(index=indexName, body=bulk_data, refresh=False)\n",
    "                            bulk_data = []\n",
    "                            totalSize = 0\n",
    "                    except:\n",
    "                        print(\"Error processing document: {}\".format(docId))\n",
    "                        raise\n",
    "\n",
    "        if len(bulk_data) > 0:\n",
    "            # index the remainder files\n",
    "            res = es.bulk(index=indexName, body=bulk_data, refresh=False)\n",
    "\n",
    "        print(\"File {0} Completed, Duration: {1} sec, Total: {2}, Processed: {3}, Empty: {4}, Variance: {5}\".\n",
    "               format(fname, time.time() - lapTime, WarcTotalDocuments, str(i), str(EmptyDocuments),\n",
    "                      str(int(WarcTotalDocuments) - i - EmptyDocuments)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traverse all the folders in the original corpus path and parallely process all gzipped warc files. \n",
    "Allow a couple of minutes or so for the collection to be indexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warcFolder = glob.glob(warcPath + \"*\")\n",
    "for warcFold in warcFolder:\n",
    "    folders = glob.glob(warcFold + \"/*\")\n",
    "    print(\"Processing Path: {}\".format(warcFold))\n",
    "\n",
    "    for fold in folders:\n",
    "        print(\"Processing folder: {}\".format(fold))\n",
    "        p = multiprocessing.Pool()\n",
    "        resultString = p.map(es_index, glob.glob(fold + \"/*\"))\n",
    "        p.close()\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the new index for ClueWeb12 with our custom similarity measure has been created, we can go and query it.\n",
    "\n",
    "#### Exercise 1 -- extension of  exercise 2 from acitivity 3\n",
    "\n",
    "Let's start puttign things together.\n",
    "Create code to produce *TREC formatted runs* for the queries below. Run these queries on the ClueWeb12_sample index we have just created and that uses your scripted similarity measure (clueweb12_custom). Have the maximum number of retrieved documents set to 20.\n",
    "\n",
    "query 251: identifying spider bites\n",
    "query 256: patron saint of mental illness\n",
    "query 258: hip roof\n",
    "\n",
    "These are the same queries you used in exercise 2 from acitivity 3 against the ClueWeb12_sample index with the default similarity function (BM25). Compare the results you have produced here with those from exercise 2 in acitivity 3. \n",
    "\n",
    "(After you have learnt to evaluate IR systems in the Evaluation session at AFIRM 2019, you can get back to these results and actually compare the runs and methods with respect to IR measures -- you will need rlevance assessments to do this: for these 3 queries, you can use the TREC Web Track 2014 qrels. You can find not official ones [here](https://github.com/zuacubd/trec-web-track-2009-2014/tree/master/2014); official ones are hosted at NIST, but its website is down due to the US Governemnt shutdown in these days.)\n",
    "\n",
    "\n",
    "#### Exercise 2\n",
    "\n",
    "Implement [language model with two stage smoothing](http://sifaka.cs.uiuc.edu/czhai/pub/sigir2002-twostage.pdf) by Zhai and Lafferty, SIGIR 2002. This language model is a combination of the Jelinek-Mercer smoothing and the Dirichlet smoothing. The formula to implement is the last in Section 3 of that paper. Leave mu and lambda as settable parameter (i.e. you do not need to consider the methods described in Section 4 for the estimation of those parameters).\n",
    "\n",
    "* Run the queries from exercise 1, setting mu=2000 and lambda=0.75. Explore how different the results are from those obtained by BM25 and TF-IDF\n",
    "* Explore the effect different settings of the paramters mu and lambda have on the ranking of the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
