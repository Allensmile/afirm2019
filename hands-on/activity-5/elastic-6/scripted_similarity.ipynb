{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Activity 5: Implementation of a Custom Similarity Model in Elasticsearch\n",
    "\n",
    "In this activity, we will implement a custom similarity model for our ClueWeb12 sample collection. \n",
    "Note that in Elasticsearch the similarity function needs to be embedded in the index: thus we will create a new index for the ClueWeb12 sample dataset.\n",
    "The pre-requisites and most scripts in this activity are the same as in Activity 1. \n",
    " \n",
    "Elasticsearch 6.5.x allows us to easily define a custom similarity by defining a scripted similarity, unlike previous versions of Elasticsearch in which a custom similarity plugin needed to be implemented as a Java class.\n",
    "\n",
    "In this activity, we are going to implement a basic TF-IDF similarity function. (note, of course, Elasticsearch already has a TF-IDF similarity function, but TF-IDF provides us a simple use case for practicing implementing a similarity functions through scripts in Elasticsearch).\n",
    "\n",
    "Let's start by importing the libraries required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import builtins\n",
    "#vars(globals()['__builtin__']) is vars(builtins)\n",
    "import gzip\n",
    "import warc\n",
    "import time\n",
    "import glob\n",
    "import lxml.html\n",
    "import re\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "from elasticsearch import Elasticsearch\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to specify the location of the Clueweb12 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put here the path to the Clueweb12B_sample file. \n",
    "#It is best you give the full, absolute path.\n",
    "warcPath = \"~/Clueweb12B_sample/\"\n",
    "\n",
    "# Check the warc Path\n",
    "if not os.path.isdir(warcPath):\n",
    "    print(\"invalid warc path\")\n",
    "else:\n",
    "    print(\"valid warc path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open connection to Elasticsearch (ES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es0 = Elasticsearch(urls='http://localhost', port=9200, timeout=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the bulk size and max documents to process in bulk (For faster indexing, we will index documents in bulks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_size = 4000\n",
    "bulk_count = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a new index name and document type for the clueweb index with the new similarity function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexName = \"clueweb12_custom\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define our custom similarity named `scripted_tfidf`. The two fields in the index (`title` and `body`) will use this custom similarity.\n",
    "\n",
    "Note that the `weight_script` will compute  the document-independent part of the score and will be available under the `weight` variable. When no `weight_script` is provided, the weight is equal to 1 by default. The `weight_script` has access to the same variables as the `script`, except `doc` since it is supposed to compute a document-independent contribution to the score.\n",
    "\n",
    "*This is really the only part of the indexing that differs from what we have already seen in activity 1.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_body = {\n",
    "    \"settings\": {\n",
    "      \"number_of_shards\": 1,\n",
    "      \"number_of_replicas\": 0,\n",
    "      \"analysis\": {\n",
    "        \"analyzer\": {\n",
    "            \"my_english\": {\n",
    "                \"tokenizer\": \"standard\",\n",
    "                \"filter\": [\"lowercase\", \"terrier_stopwords\", \"porter_stem\"]\n",
    "            }\n",
    "        },\n",
    "        \"filter\": {\n",
    "          \"terrier_stopwords\": {\n",
    "              \"type\": \"stop\",\n",
    "              \"stopwords_path\": \"stopwords/terrier-stop.txt\"\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"similarity\": {\n",
    "        \"scripted_tfidf\": {\n",
    "            \"type\": \"scripted\",\n",
    "            \"weight_script\": {\n",
    "              \"source\": \"double idf = Math.log((field.docCount+1.0)/(term.docFreq+1.0)) + 1.0; return query.boost * idf;\"\n",
    "            },\n",
    "            \"script\": {\n",
    "              \"source\": \"double tf = Math.sqrt(doc.freq); double norm = 1/Math.sqrt(doc.length); return weight * tf * norm;\"\n",
    "            }\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "      \"_doc\": {\n",
    "        \"_source\": {\n",
    "            \"enabled\": True\n",
    "        },\n",
    "        \"properties\": {\n",
    "            \"title\": {\n",
    "                 \"type\": \"text\",\n",
    "                 \"similarity\": \"scripted_tfidf\",\n",
    "                 \"analyzer\": \"my_english\"\n",
    "            },\n",
    "            \"body\": {\n",
    "                \"type\": \"text\",\n",
    "                \"similarity\": \"scripted_tfidf\",\n",
    "                \"analyzer\": \"my_english\"\n",
    "            }\n",
    "         }\n",
    "      }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the index based on the specified settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not es0.indices.exists(indexName):\n",
    "    print (\"creating \", indexName, \" index\")\n",
    "    res = es0.indices.create(index=indexName, body=request_body)\n",
    "    print(\" response: '%s'\" % res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the indexing function which will be executed as a parallel process.\n",
    "This function accepts path to a single gziped warc file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_index(fname):\n",
    "    i = 0\n",
    "    totalSize = 0\n",
    "    bulk_data = []\n",
    "    lapTime = time.time()\n",
    "    es = Elasticsearch(urls='http://localhost', port=9200, timeout=600)\n",
    "\n",
    "    print(\"Processing file: {}\".format(fname))\n",
    "    with gzip.open(fname, mode='rb') as gzf:\n",
    "        print(\"Extracted: {}\".format(fname))\n",
    "        WarcTotalDocuments = 0\n",
    "        EmptyDocuments = 0\n",
    "        for record in warc.WARCFile(fileobj=gzf):\n",
    "            if record.header.get('WARC-Type').lower() == 'warcinfo':\n",
    "                WarcTotalDocuments = record.header.get('WARC-Number-Of-Documents')\n",
    "\n",
    "            if record.header.get('WARC-Type').lower() == 'response':\n",
    "                docId = record.header.get('WARC-Trec-ID')\n",
    "                docString = record.payload.read().decode(\"utf-8\", \"ignore\")\n",
    "\n",
    "                htmlStart = docString.find('<html')\n",
    "                if htmlStart < 1:\n",
    "                    htmlStart = docString.find('<HTML')\n",
    "                if htmlStart < 1:\n",
    "                    htmlStart = docString.find('<Html')\n",
    "\n",
    "                if htmlStart < 1:\n",
    "                    EmptyDocuments += 1\n",
    "                else:\n",
    "                    # extract and scrub html string\n",
    "                    htmlString = docString[htmlStart:]\n",
    "                    htmlString = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\xff]', '', htmlString)\n",
    "                    htmlString = re.sub(r'&\\w{4,6};', '', htmlString)\n",
    "                    htmlString = htmlString.replace(\",\", \" \").replace(\"-\", \" \").replace(\".\", \" \")\n",
    "\n",
    "                    #fContent = io.BytesIO(str(htmlString.decode(\"utf-8\", \"ignore\")))\n",
    "                    fContent = io.BytesIO(htmlString.encode(\"utf-8\", \"ignore\"))\n",
    "\n",
    "                    try:\n",
    "                        htmlDoc = lxml.html.parse(fContent)\n",
    "\n",
    "                        # the html.xpath return an array so need to convert it to string with join method\n",
    "                        title = \" \".join(htmlDoc.xpath('//title/text()'))\n",
    "\n",
    "                        rootClean = htmlDoc.getroot()\n",
    "\n",
    "                        body = \" - \"\n",
    "                        try:\n",
    "                            body = rootClean.body.text_content()\n",
    "                            body = ' '.join(word for word in body.split() if word.isalnum())\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        content = title + body\n",
    "                        bulk_meta = {\n",
    "                            \"index\": {\n",
    "                                \"_index\": indexName,\n",
    "                                \"_type\": \"_doc\",\n",
    "                                \"_id\": docId\n",
    "                            }\n",
    "                        }\n",
    "\n",
    "                        bulk_content = {\n",
    "                            'title': title,\n",
    "                            'body': body\n",
    "                        }\n",
    "\n",
    "                        bulk_data.append(bulk_meta)\n",
    "                        bulk_data.append(bulk_content)\n",
    "                        totalSize += (sys.getsizeof(content) / 1024)  # convert from bytes to KiloBytes\n",
    "\n",
    "                        i += 1\n",
    "                        if totalSize > bulk_size or i % bulk_count == 0:\n",
    "                            res = es.bulk(index=indexName, body=bulk_data, refresh=False)\n",
    "                            bulk_data = []\n",
    "                            totalSize = 0\n",
    "                    except:\n",
    "                        print(\"Error processing document: {}\".format(docId))\n",
    "                        raise\n",
    "\n",
    "        if len(bulk_data) > 0:\n",
    "            # index the remainder files\n",
    "            res = es.bulk(index=indexName, body=bulk_data, refresh=False)\n",
    "\n",
    "        print(\"File {0} Completed, Duration: {1} sec, Total: {2}, Processed: {3}, Empty: {4}, Variance: {5}\".\n",
    "               format(fname, time.time() - lapTime, WarcTotalDocuments, str(i), str(EmptyDocuments),\n",
    "                      str(int(WarcTotalDocuments) - i - EmptyDocuments)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traverse all the folders in the original corpus path and parallely process all gzipped warc files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warcFolder = glob.glob(warcPath + \"*\")\n",
    "for warcFold in warcFolder:\n",
    "    folders = glob.glob(warcFold + \"/*\")\n",
    "    print(\"Processing Path: {}\".format(warcFold))\n",
    "\n",
    "    for fold in folders:\n",
    "        print(\"Processing folder: {}\".format(fold))\n",
    "        p = multiprocessing.Pool()\n",
    "        resultString = p.map(es_index, glob.glob(fold + \"/*\"))\n",
    "        p.close()\n",
    "        p.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
