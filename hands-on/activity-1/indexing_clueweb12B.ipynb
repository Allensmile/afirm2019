{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing Cluweb12B (used in TREC Web Track 2013-2014, CLEF2016 and CLEF2017)\n",
    "This notebook describes how to index the a sample of the ClueWeb12 corpus using Elasticsearch.\n",
    "The index will have two fields: title and body.\n",
    "Each field will have its own custom similarity function to enable similarity tuning.\n",
    "\n",
    "Pre-processing is applied to both fields, and includes:\n",
    "* lowercasing\n",
    "* removing stopwords based on the Terrier stopwords list\n",
    "* steeming using Porter Stemmer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Indexing using Elasticsearch and Python\n",
    "\n",
    "Next, we examine the Python code necessary to index using Elasticsearch. First, we need to import the necessary libaries:\n",
    "* gzip: to unzip Aquaint collection files\n",
    "* warc: clueweb12 corpus is in warc file\n",
    "* time: to measure running time\n",
    "* glob: to traverse all Aquaint collection files\n",
    "* re: regular experession to search unwanted phrase (e.g., special characters)\n",
    "* sys: to measure size of bulk package variable\n",
    "* elasticsearch: to work with Elasticsearch\n",
    "* lxml.html: clueweb12 warc files contains HTMLs, so we need this library to parse the HTML\n",
    "* multiprocessing: clueweb12 is a very big corpus, to make it faster, we use multiprocessing to index in parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'warc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-737c6e8f71ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwarc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlxml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'warc'"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import warc\n",
    "import time\n",
    "import glob\n",
    "import lxml.html\n",
    "import re\n",
    "import io\n",
    "import sys\n",
    "from elasticsearch import Elasticsearch\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to specify the location of the Clueweb12 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warcPath = \"~\\Clueweb12B_sample\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open connection to Elasticsearch (ES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Elasticsearch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-87fb675a600b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mes0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElasticsearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'http://localhost'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Elasticsearch' is not defined"
     ]
    }
   ],
   "source": [
    "es0 = Elasticsearch(urls='http://localhost', port=9200, timeout=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify bulk size and max documents in a bulk. For faster indexing, we will index documents in bulks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_size = 4000\n",
    "bulk_count = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define index name and document type for the aquaint index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexName = \"clueweb12_sample\"\n",
    "docType = \"clueweb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the index settings and mappings. Variable definition:\n",
    "* \"number_of_shards\": 1 --> all documents will be indexed in one shard (i.e. no partitions)\n",
    "* \"number_of_replicas\": 0 --> no replica (i.e. no back up index)\n",
    "* \"my_english\" --> custom analyzer telling ES to use the \"standard\" english tokenizer, lowercase all characters, remove stopwords based on the custom \"terrier_stopwords\", and stem using Porter stemmer.\n",
    "* \"terrier_stopwords\" --> custom stopwords list based on the stopwords/terrier-stop.txt file\n",
    "* \"similarity\" --> custom similarity function *for each field*. Here we are using BM25 and we set the BM25 parameters to standard values for both fields.\n",
    "* \"_source\" --> specify to store or not to store the document text in the index (False means not storing the document text)\n",
    "* \"properties\" --> field definition; utilizes the defined custom similarity and analyzer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_body = {\n",
    "    \"settings\": {\n",
    "      \"number_of_shards\": 1,\n",
    "      \"number_of_replicas\": 0,\n",
    "      \"analysis\": {\n",
    "        \"analyzer\": {\n",
    "            \"my_english\": {\n",
    "                \"tokenizer\": \"standard\",\n",
    "                \"filter\": [\"lowercase\", \"terrier_stopwords\", \"porter_stem\"]\n",
    "            }\n",
    "        },\n",
    "        \"filter\": {\n",
    "          \"terrier_stopwords\": {\n",
    "              \"type\": \"stop\",\n",
    "              \"stopwords_path\": \"stopwords/terrier-stop.txt\"\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"similarity\": {\n",
    "        \"sim_title\": {\n",
    "            \"type\": \"BM25\",\n",
    "            \"b\": 0.75,\n",
    "            \"k1\": 1.2\n",
    "        },\n",
    "        \"sim_body\": {\n",
    "            \"type\": \"BM25\",\n",
    "            \"b\": 0.75,\n",
    "            \"k1\": 1.2\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "      docType: {\n",
    "        \"_source\": {\n",
    "            \"enabled\": False\n",
    "        },\n",
    "        \"properties\": {\n",
    "            \"title\": {\n",
    "                 \"type\": \"text\",\n",
    "                 \"similarity\": \"sim_title\",\n",
    "                 \"analyzer\": \"my_english\"\n",
    "            },\n",
    "            \"body\": {\n",
    "                \"type\": \"text\",\n",
    "                \"similarity\": \"sim_body\",\n",
    "                \"analyzer\": \"my_english\"\n",
    "            }\n",
    "         }\n",
    "      }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the index based on the specified settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'es0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ade8237a74ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mes0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"creating \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" index, start at \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstartTime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mes0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" response: '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'es0' is not defined"
     ]
    }
   ],
   "source": [
    "if not es0.indices.exists(indexName):\n",
    "    print (\"creating \", indexName, \" index, start at \", startTime)\n",
    "    res = es0.indices.create(index=indexName, body=request_body)\n",
    "    print(\" response: '%s'\" % res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the indexing function which will be executed as a parallel process.\n",
    "This function accepts path to a single gziped warc file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_index(fname):\n",
    "    i = 0\n",
    "    totalSize = 0\n",
    "    bulk_data = []\n",
    "    lapTime = time.time()\n",
    "    es = Elasticsearch(urls='http://localhost', port=9200, timeout=600)\n",
    "\n",
    "    print(\"Processing file: {}\".format(fname))\n",
    "    with gzip.open(fname, mode='rb') as gzf:\n",
    "        WarcTotalDocuments = 0\n",
    "        EmptyDocuments = 0\n",
    "        for record in warc.WARCFile(fileobj=gzf):\n",
    "            if record.header.get('WARC-Type').lower() == 'warcinfo':\n",
    "                WarcTotalDocuments = record.header.get('WARC-Number-Of-Documents')\n",
    "\n",
    "            if record.header.get('WARC-Type').lower() == 'response':\n",
    "                docId = record.header.get('WARC-Trec-ID')\n",
    "                docString = record.payload.read()\n",
    "\n",
    "                htmlStart = docString.find('<html')\n",
    "                if htmlStart < 1:\n",
    "                    htmlStart = docString.find('<HTML')\n",
    "                if htmlStart < 1:\n",
    "                    htmlStart = docString.find('<Html')\n",
    "\n",
    "                if htmlStart < 1:\n",
    "                    EmptyDocuments += 1\n",
    "                else:\n",
    "                    # extract and scrub html string\n",
    "                    htmlString = docString[htmlStart:]\n",
    "                    htmlString = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\xff]', '', htmlString)\n",
    "                    htmlString = re.sub(r'&\\w{4,6};', '', htmlString)\n",
    "                    htmlString = htmlString.replace(\",\", \" \").replace(\"-\", \" \").replace(\".\", \" \")\n",
    "\n",
    "                    fContent = io.BytesIO(str(htmlString.decode(\"utf-8\", \"ignore\")))\n",
    "\n",
    "                    try:\n",
    "                        htmlDoc = lxml.html.parse(fContent)\n",
    "\n",
    "                        # the html.xpath return an array so need to convert it to string with join method\n",
    "                        title = \" \".join(htmlDoc.xpath('//title/text()'))\n",
    "\n",
    "                        rootClean = htmlDoc.getroot()\n",
    "\n",
    "                        body = \" - \"\n",
    "                        try:\n",
    "                            body = rootClean.body.text_content()\n",
    "                            body = ' '.join(word for word in body.split() if word.isalnum())\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        content = title + body\n",
    "                        bulk_meta = {\n",
    "                            \"index\": {\n",
    "                                \"_index\": indexName,\n",
    "                                \"_type\": docType,\n",
    "                                \"_id\": docId\n",
    "                            }\n",
    "                        }\n",
    "\n",
    "                        bulk_content = {\n",
    "                            'title': title,\n",
    "                            'body': body\n",
    "                        }\n",
    "\n",
    "                        bulk_data.append(bulk_meta)\n",
    "                        bulk_data.append(bulk_content)\n",
    "                        totalSize += (sys.getsizeof(content) / 1024)  # convert from bytes to KiloBytes\n",
    "\n",
    "                        i += 1\n",
    "                        if totalSize > bulk_size or i % bulk_count == 0:\n",
    "                            res = es.bulk(index=indexName, doc_type=docType, body=bulk_data, refresh=False)\n",
    "                            bulk_data = []\n",
    "                            totalSize = 0\n",
    "                    except:\n",
    "                        print(\"Error processing document: {}\".format(docId))\n",
    "                        raise\n",
    "\n",
    "        if len(bulk_data) > 0:\n",
    "            # index the remainder files\n",
    "            res = es.bulk(index=indexName, doc_type=docType, body=bulk_data, refresh=False)\n",
    "\n",
    "        print(\"File {0} Completed, Duration: {1} sec, Total: {2}, Processed: {3}, Empty: {4}, Variance: {5}\".\n",
    "               format(fname, time.time() - lapTime, WarcTotalDocuments, str(i), str(EmptyDocuments),\n",
    "                      str(int(WarcTotalDocuments) - i - EmptyDocuments)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traverse all the folders in the original corpus path and parallely process all gzipped warc files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warcFolder = glob.glob(warcPath + \"*\")\n",
    "for warcFold in warcFolder:\n",
    "    folders = glob.glob(warcFold + \"/*\")\n",
    "    print(\"Processing Path: {}\".format(warcFold))\n",
    "\n",
    "    for fold in folders:\n",
    "        print(\"Processing folder: {}\".format(fold))\n",
    "        p = multiprocessing.Pool()\n",
    "        resultString = p.map(es_index, glob.glob(fold + \"/*\"))\n",
    "        p.close()\n",
    "        p.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
